# -*- coding: utf-8 -*-
"""
Online Learning Service - FULL IMPLEMENTATION
GerÃ§ek incremental model retraining + scheduled jobs
"""

import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import os
import numpy as np

logger = logging.getLogger(__name__)

try:
    from apscheduler.schedulers.background import BackgroundScheduler
    from apscheduler.triggers.cron import CronTrigger
    SCHEDULER_AVAILABLE = True
except ImportError:
    SCHEDULER_AVAILABLE = False
    logger.warning("âš ï¸ APScheduler not available, scheduled jobs disabled")


class OnlineLearningService:
    """
    ğŸ”„ Online Learning: Incremental Model Updates
    
    Features:
    - Scheduled model retraining (haftalÄ±k)
    - Incremental fine-tuning with new data
    - Performance validation (rollback if worse)
    - Material statistics tracking
    - Automatic model versioning
    """
    
    def __init__(self):
        self.last_update = datetime.now()
        self.update_interval = timedelta(days=7)  # HaftalÄ±k
        self.material_stats = {}  # Material istatistikleri
        self.scheduler = None
        self.update_history = []  # Update logs
        
        # Performance tracking
        self.baseline_metrics = {
            'power_mae': float('inf'),
            'speed_mae': float('inf'),
            'passes_mae': float('inf')
        }
        
        logger.info("âœ… OnlineLearningService initialized")
        
        # Start scheduler
        if SCHEDULER_AVAILABLE:
            self._start_scheduler()
    
    def _start_scheduler(self):
        """Start background scheduler for weekly updates"""
        try:
            self.scheduler = BackgroundScheduler()
            
            # HaftalÄ±k update (Her Pazar 02:00)
            self.scheduler.add_job(
                self.weekly_model_update,
                trigger=CronTrigger(day_of_week='sun', hour=2, minute=0),
                id='weekly_model_update',
                name='Weekly Model Retraining',
                replace_existing=True
            )
            
            # GÃ¼nlÃ¼k istatistik gÃ¼ncelleme (Her gÃ¼n 03:00)
            self.scheduler.add_job(
                self.update_material_statistics,
                trigger=CronTrigger(hour=3, minute=0),
                id='daily_stats_update',
                name='Daily Statistics Update',
                replace_existing=True
            )
            
            self.scheduler.start()
            logger.info("âœ… Scheduler started: Weekly retraining enabled")
            
        except Exception as e:
            logger.error(f"âŒ Scheduler start failed: {e}")
    
    def should_update(self) -> bool:
        """GÃ¼ncelleme zamanÄ± geldi mi?"""
        return datetime.now() - self.last_update > self.update_interval
    
    def weekly_model_update(self):
        """
        ğŸ”„ HaftalÄ±k model gÃ¼ncelleme (SCHEDULED JOB)
        
        Steps:
        1. Yeni deneyleri Firebase'den Ã§ek (son 7 gÃ¼n)
        2. Yeterli veri varsa model fine-tune et
        3. Test set'te performans kontrol et
        4. Ä°yiyse deploy, kÃ¶tÃ¼yse rollback
        """
        from firebase_service import get_firebase_service
        from ml_transfer_model import get_transfer_model, TF_AVAILABLE
        from ml_feature_engineering import get_feature_encoder
        from model_storage_service import get_storage_service
        from image_preprocessing_service import get_image_preprocessor
        
        logger.info("=" * 80)
        logger.info("ğŸ”„ WEEKLY MODEL UPDATE STARTED")
        logger.info("=" * 80)
        
        if not TF_AVAILABLE:
            logger.warning("âš ï¸ TensorFlow not available, skipping update")
            return
        
        firebase = get_firebase_service()
        if not firebase.is_available():
            logger.warning("âš ï¸ Firebase not available, skipping update")
            return
        
        try:
            # 1. Get new experiments (last 7 days)
            cutoff_date = datetime.now() - timedelta(days=7)
            all_experiments = firebase.get_training_data_for_transfer_learning(limit=500)
            
            # Filter son 7 gÃ¼n (Firebase'de timestamp varsa)
            # Åimdilik tÃ¼m verileri kullan
            new_experiments = all_experiments
            
            logger.info(f"ğŸ“Š Found {len(new_experiments)} training samples")
            
            if len(new_experiments) < 10:
                logger.warning("âš ï¸ Insufficient new data (need 10+), skipping update")
                return
            
            # 2. Prepare data
            feature_encoder = get_feature_encoder()
            image_preprocessor = get_image_preprocessor()
            
            X_num, y_power, y_speed, y_passes = feature_encoder.encode_batch(new_experiments)
            
            # Images (if available)
            X_img = None
            if image_preprocessor.is_available():
                logger.info("ğŸ“¸ Loading images for training...")
                # Firebase'den gerÃ§ek experiment dict'leri Ã§ek
                firebase_experiments = [
                    firebase.db.collection('experiments').document(exp.get('id', 'unknown')).get().to_dict()
                    for exp in new_experiments[:50]  # Ä°lk 50'yi dene
                    if exp.get('id')
                ]
                
                if len(firebase_experiments) > 0:
                    X_img = image_preprocessor.load_batch_images(
                        firebase_experiments, 
                        augment=True,
                        fallback_to_zeros=True
                    )
                    logger.info(f"âœ… Loaded {len(X_img)} images")
            
            # 3. Load current model
            transfer_model = get_transfer_model()
            if not transfer_model or not transfer_model.is_trained:
                logger.warning("âš ï¸ No trained model to update")
                return
            
            # 4. Evaluate current model (baseline)
            logger.info("ğŸ“Š Evaluating current model...")
            
            # Test set split (20% of new data)
            split_idx = int(len(X_num) * 0.8)
            X_num_train, X_num_test = X_num[:split_idx], X_num[split_idx:]
            y_power_train, y_power_test = y_power[:split_idx], y_power[split_idx:]
            y_speed_train, y_speed_test = y_speed[:split_idx], y_speed[split_idx:]
            y_passes_train, y_passes_test = y_passes[:split_idx], y_passes[split_idx:]
            
            if X_img is not None:
                X_img_train, X_img_test = X_img[:split_idx], X_img[split_idx:]
            else:
                X_img_train, X_img_test = None, None
            
            old_metrics = transfer_model.evaluate(
                X_num_test, y_power_test, y_speed_test, y_passes_test, X_img_test
            )
            
            old_loss = old_metrics.get('loss', float('inf'))
            logger.info(f"ğŸ“Š Current model loss: {old_loss:.4f}")
            
            # 5. Fine-tune model
            logger.info("ğŸ”§ Fine-tuning model with new data...")
            
            history = transfer_model.fine_tune(
                X_num_train, y_power_train, y_speed_train, y_passes_train,
                X_images=X_img_train,
                freeze_cnn=True,
                epochs=30
            )
            
            # 6. Evaluate new model
            logger.info("ğŸ“Š Evaluating updated model...")
            
            new_metrics = transfer_model.evaluate(
                X_num_test, y_power_test, y_speed_test, y_passes_test, X_img_test
            )
            
            new_loss = new_metrics.get('loss', float('inf'))
            logger.info(f"ğŸ“Š Updated model loss: {new_loss:.4f}")
            
            # 7. Validation: Deploy or Rollback
            improvement_threshold = 1.2  # Max 20% worse
            
            if new_loss <= old_loss * improvement_threshold:
                logger.info("âœ… Model improved or acceptable, deploying...")
                
                # Save locally
                local_model_path = "models/diode_laser_transfer_v1.h5"
                os.makedirs("models", exist_ok=True)
                transfer_model.save_model(local_model_path)
                
                # Upload to Firebase Storage
                storage_service = get_storage_service()
                if storage_service.is_available():
                    success = storage_service.save_model_to_storage(local_model_path)
                    
                    if success:
                        logger.info("âœ… Model deployed to Firebase Storage")
                        
                        # Log update
                        self.update_history.append({
                            'timestamp': datetime.now().isoformat(),
                            'samples': len(new_experiments),
                            'old_loss': old_loss,
                            'new_loss': new_loss,
                            'improvement': (old_loss - new_loss) / old_loss * 100,
                            'status': 'deployed'
                        })
                        
                        self.last_update = datetime.now()
                    else:
                        logger.error("âŒ Failed to upload to Firebase")
                else:
                    logger.warning("âš ï¸ Firebase Storage not available")
                
            else:
                logger.warning(f"âš ï¸ Model degraded ({new_loss:.4f} > {old_loss:.4f}), ROLLBACK")
                
                # Reload previous model from Firebase
                storage_service = get_storage_service()
                if storage_service.is_available():
                    local_model_path = "models/diode_laser_transfer_v1.h5"
                    storage_service.load_model_from_storage(local_model_path)
                    transfer_model.load_model(local_model_path)
                    logger.info("âœ… Rolled back to previous model")
                
                # Log failed update
                self.update_history.append({
                    'timestamp': datetime.now().isoformat(),
                    'samples': len(new_experiments),
                    'old_loss': old_loss,
                    'new_loss': new_loss,
                    'degradation': (new_loss - old_loss) / old_loss * 100,
                    'status': 'rollback'
                })
            
            logger.info("=" * 80)
            logger.info("âœ… WEEKLY MODEL UPDATE COMPLETED")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"âŒ Weekly update error: {e}")
            logger.exception("Full error:")
    
    def update_material_statistics(self):
        """
        ğŸ“Š GÃ¼nlÃ¼k istatistik gÃ¼ncelleme (SCHEDULED JOB)
        
        Material-based averages gÃ¼ncelle
        """
        from firebase_service import get_firebase_service
        
        if not self.should_update():
            logger.info("â³ Update interval not reached yet")
            return
        
        firebase = get_firebase_service()
        if not firebase.is_available():
            logger.warning("âš ï¸ Firebase not available for statistics update")
            return
        
        try:
            logger.info("ğŸ“Š Updating material statistics...")
            
            # TÃ¼m verified deneyleri Ã§ek
            all_experiments = firebase.get_all_verified_experiments(
                limit=1000,
                only_diode=True
            )
            
            if len(all_experiments) == 0:
                logger.warning("âš ï¸ No experiments for statistics update")
                return
            
            # Material bazÄ±nda istatistik gÃ¼ncelle
            materials = self._get_unique_materials(all_experiments)
            updated_count = 0
            
            for material in materials:
                material_exps = [
                    e for e in all_experiments 
                    if e.get('materialType', '').lower() == material.lower()
                ]
                
                if len(material_exps) >= 5:
                    stats = self._calculate_material_stats(material_exps)
                    self.material_stats[material] = stats
                    updated_count += 1
                    logger.debug(f"âœ… Updated '{material}': {len(material_exps)} experiments")
            
            logger.info(f"âœ… Statistics updated for {updated_count} materials")
            
        except Exception as e:
            logger.error(f"âŒ Statistics update error: {e}")
    
    def _get_unique_materials(self, experiments: List[Dict]) -> List[str]:
        """Unique material types"""
        return list(set(e.get('materialType', 'Unknown') for e in experiments))
    
    def _calculate_material_stats(self, experiments: List[Dict]) -> Dict:
        """
        Bir malzeme iÃ§in istatistikler
        
        Returns:
            {
                'avg_power': float,
                'avg_speed': float,
                'avg_passes': float,
                'count': int,
                'avg_quality': float
            }
        """
        cutting_exps = [e for e in experiments if 'cutting' in e.get('processes', {})]
        
        if len(cutting_exps) == 0:
            return {}
        
        powers = [e['processes']['cutting']['power'] for e in cutting_exps]
        speeds = [e['processes']['cutting']['speed'] for e in cutting_exps]
        passes = [e['processes']['cutting']['passes'] for e in cutting_exps]
        qualities = [e.get('qualityScores', {}).get('cutting', 5) for e in cutting_exps]
        
        return {
            'avg_power': sum(powers) / len(powers),
            'avg_speed': sum(speeds) / len(speeds),
            'avg_passes': sum(passes) / len(passes),
            'count': len(cutting_exps),
            'avg_quality': sum(qualities) / len(qualities),
            'last_updated': datetime.now().isoformat()
        }
    
    def get_material_baseline(self, material: str) -> Dict:
        """
        Bir malzeme iÃ§in baseline parametreler
        Online learning ile gÃ¼ncellenen deÄŸerler
        """
        return self.material_stats.get(material, {})
    
    def get_update_history(self, limit: int = 10) -> List[Dict]:
        """Get recent update history"""
        return self.update_history[-limit:]
    
    def manual_update_trigger(self):
        """
        Manuel gÃ¼ncelleme tetikle (admin endpoint iÃ§in)
        """
        logger.info("ğŸ”§ Manual update triggered")
        self.weekly_model_update()
    
    def shutdown(self):
        """Shutdown scheduler"""
        if self.scheduler:
            self.scheduler.shutdown()
            logger.info("ğŸ›‘ Scheduler stopped")


# Global singleton
_online_learner = None

def get_online_learner() -> OnlineLearningService:
    """Get or create online learner singleton"""
    global _online_learner
    if _online_learner is None:
        _online_learner = OnlineLearningService()
    return _online_learner